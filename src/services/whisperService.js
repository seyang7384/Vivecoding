class WhisperService {
    constructor() {
        this.mediaRecorder = null;
        this.audioChunks = [];
        this.isRecording = false;

        // Medical terminology hints for better accuracy
        this.medicalHints = "ë°±í•˜ìˆ˜ì˜¤, ìžê°ì´ˆ, ë‹¹ê·€, íƒ•ì „ì‹¤, ë°œì¹¨, ì•½ì¹¨, ì¶”ë‚˜, ì˜ˆì§„, ì†¡ë¯¸ë ¹, í™©ê¸°, ì¸ì‚¼, ì²œê¶, ë°±ì¶œ, ë°±ë³µë ¹, ì§„í”¼, ë°˜í•˜, ê°ì´ˆ, ìƒê°•, ëŒ€ì¡°";
    }

    async startRecording() {
        try {
            // Check if mediaDevices is supported
            if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
                throw new Error('ì´ ë¸Œë¼ìš°ì €ëŠ” ì˜¤ë””ì˜¤ ë…¹ìŒì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. Chromeì´ë‚˜ Edgeë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”.');
            }

            console.log('ðŸŽ¤ Requesting microphone access...');
            const stream = await navigator.mediaDevices.getUserMedia({
                audio: {
                    echoCancellation: true,
                    noiseSuppression: true,
                    sampleRate: 44100
                }
            });

            console.log('âœ… Microphone access granted');

            // Use audio/webm for better browser compatibility
            const mimeType = MediaRecorder.isTypeSupported('audio/webm')
                ? 'audio/webm'
                : 'audio/mp4';

            console.log('ðŸ“¼ Recording format:', mimeType);

            this.mediaRecorder = new MediaRecorder(stream, { mimeType });
            this.audioChunks = [];

            this.mediaRecorder.ondataavailable = (event) => {
                if (event.data.size > 0) {
                    this.audioChunks.push(event.data);
                    console.log('ðŸ“¦ Audio chunk received:', event.data.size, 'bytes');
                }
            };

            this.mediaRecorder.start();
            this.isRecording = true;

            console.log('ðŸ”´ Recording started');
            return true;
        } catch (error) {
            console.error('âŒ Failed to start recording:', error);

            if (error.name === 'NotAllowedError' || error.name === 'PermissionDeniedError') {
                throw new Error('ë§ˆì´í¬ ê¶Œí•œì´ ê±°ë¶€ë˜ì—ˆìŠµë‹ˆë‹¤. ë¸Œë¼ìš°ì € ì„¤ì •ì—ì„œ ë§ˆì´í¬ ê¶Œí•œì„ í—ˆìš©í•´ì£¼ì„¸ìš”.');
            } else if (error.name === 'NotFoundError') {
                throw new Error('ë§ˆì´í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë§ˆì´í¬ê°€ ì—°ê²°ë˜ì–´ ìžˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.');
            }

            throw new Error('ë§ˆì´í¬ ì ‘ê·¼ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤: ' + error.message);
        }
    }

    async stopRecording() {
        return new Promise((resolve, reject) => {
            if (!this.mediaRecorder || !this.isRecording) {
                reject(new Error('ë…¹ìŒì´ ì‹œìž‘ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'));
                return;
            }

            console.log('â¹ï¸ Stopping recording...');

            this.mediaRecorder.onstop = async () => {
                const audioBlob = new Blob(this.audioChunks, { type: this.mediaRecorder.mimeType });

                console.log('âœ… Recording stopped. Audio size:', audioBlob.size, 'bytes');

                // Stop all tracks
                this.mediaRecorder.stream.getTracks().forEach(track => {
                    track.stop();
                    console.log('ðŸ›‘ Track stopped:', track.kind);
                });

                this.isRecording = false;
                resolve(audioBlob);
            };

            this.mediaRecorder.stop();
        });
    }

    async transcribe(audioBlob) {
        const apiKey = import.meta.env.VITE_OPENAI_API_KEY;

        console.log('ðŸ¤– Starting transcription...');
        console.log('ðŸ”‘ API Key configured:', apiKey ? 'Yes' : 'No (using mock)');

        if (!apiKey || apiKey === 'your_openai_api_key_here') {
            // Development fallback - return mock transcription
            console.warn('âš ï¸ OpenAI API key not configured. Using mock transcription.');
            await new Promise(resolve => setTimeout(resolve, 1000)); // Simulate API delay
            const mockText = 'ì†¡ë¯¸ë ¹ë‹˜ ì¹¨ ì¹˜ë£Œ ëë‚¬ìŠµë‹ˆë‹¤.';
            console.log('âœ… Mock transcription:', mockText);
            return mockText;
        }

        try {
            const formData = new FormData();

            // Convert blob to file with proper extension
            const audioFile = new File([audioBlob], 'audio.webm', {
                type: audioBlob.type
            });

            console.log('ðŸ“¤ Sending to Whisper API...');
            console.log('ðŸ“ File size:', audioFile.size, 'bytes');
            console.log('ðŸ’¬ Medical hints:', this.medicalHints.substring(0, 50) + '...');

            formData.append('file', audioFile);
            formData.append('model', 'whisper-1');
            formData.append('language', 'ko'); // Korean
            formData.append('prompt', this.medicalHints); // Medical terminology hints

            const response = await fetch('https://api.openai.com/v1/audio/transcriptions', {
                method: 'POST',
                headers: {
                    'Authorization': `Bearer ${apiKey}`
                },
                body: formData
            });

            if (!response.ok) {
                const error = await response.json();
                console.error('âŒ Whisper API error:', error);
                throw new Error(error.error?.message || 'Whisper API ì˜¤ë¥˜');
            }

            const data = await response.json();
            console.log('âœ… Whisper transcription:', data.text);
            return data.text;
        } catch (error) {
            console.error('âŒ Whisper transcription error:', error);
            throw new Error('ìŒì„± ì¸ì‹ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: ' + error.message);
        }
    }

    cancelRecording() {
        if (this.mediaRecorder && this.isRecording) {
            this.mediaRecorder.stream.getTracks().forEach(track => track.stop());
            this.isRecording = false;
            this.audioChunks = [];
            console.log('ðŸš« Recording cancelled');
        }
    }
}

export const whisperService = new WhisperService();
